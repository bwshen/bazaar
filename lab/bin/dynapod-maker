#!/bin/bash -e
#
# This script must be executed from an SDMAIN_ROOT directory with compiled
# code, e.g. from the root of an unpacked tarball.
#
# A typical invocation from Jenkins would look like this:
#    RKRELEASE=`~/sdmain/src/scripts/jenkins/unpack_build.sh`
#    ./src/scripts/jenkins/jenkins_rktool.py --set_build_display_name ${BUILD_URL} ${RESERVED_RESOURCE}-${BUILD_NUMBER}
#    cd $RKRELEASE
#    dynapod-maker ${RESERVED_RESOURCE}    # e.g. 'dynapod1.yml'
#

#
# TODO(adi/sandeep/rahul):
#  - Refactor this to a proper script
#  - Pass parameters via CLI as opposed to environment variables
#  - Convert to Python
#
# This script was originally copied pretty much "as is" from the Jenkins
# 'recover-dynamic-pod' job.  No effort was made to clean it up at the time.
#

usage() {
    echo "Usage: $0 <reserved-resource>"
    echo "   or: $0 <pre-build-dir> <merge-rktest-yamls-tool-dir> <podmaker-dir> <cluster-bootstrap-tool-dir> <jenkins-rktool-dir> <reserved-resource>"
}

#
# Temporary...  :-)
#
# TODO(adi): clean this up once we know what should run from where.
#
# We allow the caller to specify the path to each one of the tools being called
# from this script.  This gives us maximum flexibility of changing paths on
# the fly from jenkins jobs.
#
# For backwards compatibility we support invocation with just a single
# argument.
#
PRE_BUILD_DIR="."
MERGE_RKTEST_YAMLS_TOOL_DIR="."
PODMAKER_DIR="."
CLUSTER_BOOTSTRAP_TOOL_DIR="."
JENKINS_RKTOOL_DIR="."
if [ $# -eq 7 ]; then
    PRE_BUILD_DIR="$1"
    MERGE_RKTEST_YAMLS_TOOL_DIR="$2"
    PODMAKER_DIR="$3"
    CLUSTER_BOOTSTRAP_TOOL_DIR="$4"
    JENKINS_RKTOOL_DIR="$5"
    FTP_UTIL_DIR="$6"
    shift 6
fi

if [ $# -ne 1 ]; then
    usage
    exit 1
fi

export RESERVED_RESOURCE=$1
export ANSIBLE_LOG_PATH="./ansible.log.txt"
export ANSIBLE_HOST_KEY_CHECKING=False
export LOG_LEVEL=DEBUG

# For example RESERVED_RESOURCE=dynapod1.yml, specs_yml=dynapod1.specs.yml
specs_yml=`echo "$RESERVED_RESOURCE" | \
    sed -re 's/^(.+)\.([^\.]+)$/\1.specs.\2/'`

#########################################################################################################################


# Get the Specs to use for spinning up a fresh pod.
${MERGE_RKTEST_YAMLS_TOOL_DIR}/src/scripts/tests/merge_rktest_yamls_tool.py \
    "${PODMAKER_DIR}/conf/${specs_yml}" --outputyaml dynapod_specs.yml

# Create DynaPod
# AD user creation is disabled for now since it doesn't quite work for the
# colo AD.
LOG_FILE="./podmaker.log.txt" ${PODMAKER_DIR}/src/py/utils/podmaker.py \
          --config=dynapod_specs.yml \
          --workdir ${CLUSTER_BOOTSTRAP_TOOL_DIR} \
          --outputfile conf/${RESERVED_RESOURCE} \
          --report_ttl \
          --skip_ad_user_creation

# The pre_build.py script of older rev code with which we normally
# pre-bootstrap dynapod clusters, does not seem to set all the deployment
# variables properly when a grafana node is available. If we SKIP_DEPLOY
# because pre_build install option was used, we will fail to set key property
# of livestat target to use. We are therefore better off skipping install
# in pre_build and do it for once and all in bootstrap tool.
${PRE_BUILD_DIR}/src/scripts/jenkins/pre_build.py -nofetch_master \
   -skip_install --resource=${RESERVED_RESOURCE}

# Enable developer mode (Ref: http://phabricator.scale-data.com/D15936)
${PRE_BUILD_DIR}/deployment/cluster.sh \
  merged_ipv6 exec nodes "echo 1 | sudo tee /var/lib/rubrik/developer_mode"

# Create Avahi override file to disable it (CDM-94063)
${PRE_BUILD_DIR}/deployment/cluster.sh \
  merged_ipv6 exec nodes "sudo mkdir -p /var/lib/rubrik/node-monitor && sudo touch /var/lib/rubrik/node-monitor/hosts_to_ipv6_mapping"

export SKIP_CHECK_PROTECTED_VMS=1
${CLUSTER_BOOTSTRAP_TOOL_DIR}/src/scripts/tests/cluster_bootstrap_tool.py \
  --skip_check_testbed --skip_stats_audit --no_stats_auditors \
  --skip_wait_for_vcenter_refresh --add_6_5_to_supported_vcenter_versions
#########################################################################################################################

# Stage the yaml and jinja files we generated into the outer directory, which
# in the case of a Jenkins job is the top level of the workspace directory
# where artifacts will ultimately be collected for publishing.
cp -vt .. ${CLUSTER_BOOTSTRAP_TOOL_DIR}/conf/${RESERVED_RESOURCE} ${CLUSTER_BOOTSTRAP_TOOL_DIR}/deployment/ansible/*dynapod* *.txt

# Upload all the dynapod files to the FTP server
echo "Dynapod resources generated by ${JOB_NAME} number ${BUILD_NUMBER} at ${BUILD_URL} " \
  > ${RESERVED_RESOURCE%%.*}.txt

${FTP_UTIL_DIR}/src/py/utils/ftp_util.py \
  --ftp_server "files-master.colo.rubrik-lab.com" \
  --ftp_user "ubuntu" \
  --ftp_password "qwerty" \
  --make_directory "Dynapod/${RESERVED_RESOURCE%%.*}" \
  --upload_file ../*dynapod* ../*.txt \
  --destination "Dynapod/${RESERVED_RESOURCE%%.*}/"
